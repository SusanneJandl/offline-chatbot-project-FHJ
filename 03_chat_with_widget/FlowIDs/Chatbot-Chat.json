{"name": "Chatbot-Chat", "description": "ATMS chatbot", "icon": null, "icon_bg_color": null, "gradient": null, "data": {"nodes": [{"id": "ChatInput-Q0ufi", "type": "genericNode", "position": {"x": -1386.5005184218498, "y": 277.68273814389573}, "data": {"type": "ChatInput", "node": {"template": {"_type": "Component", "files": {"trace_as_metadata": true, "file_path": "", "fileTypes": ["txt", "md", "mdx", "csv", "json", "yaml", "yml", "xml", "html", "htm", "pdf", "docx", "py", "sh", "sql", "js", "ts", "tsx", "jpg", "jpeg", "png", "bmp", "image"], "list": true, "required": false, "placeholder": "", "show": true, "name": "files", "value": "", "display_name": "Files", "advanced": true, "dynamic": false, "info": "Files to be sent with the message.", "title_case": false, "type": "file", "_input_type": "FileInput"}, "background_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "background_color", "value": "", "display_name": "Background Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The background color of the icon.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "chat_icon": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "chat_icon", "value": "", "display_name": "Icon", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The icon of the message.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.data.utils import IMG_FILE_TYPES, TEXT_FILE_TYPES\nfrom langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, FileInput, MessageTextInput, MultilineInput, Output\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_USER, MESSAGE_SENDER_USER\n\n\nclass ChatInput(ChatComponent):\n    display_name = \"Chat Input\"\n    description = \"Get chat inputs from the Playground.\"\n    icon = \"MessagesSquare\"\n    name = \"ChatInput\"\n\n    inputs = [\n        MultilineInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            value=\"\",\n            info=\"Message to be passed as input.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_USER,\n            info=\"Type of sender.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_USER,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        FileInput(\n            name=\"files\",\n            display_name=\"Files\",\n            file_types=TEXT_FILE_TYPES + IMG_FILE_TYPES,\n            info=\"Files to be sent with the message.\",\n            advanced=True,\n            is_list=True,\n        ),\n        MessageTextInput(\n            name=\"background_color\",\n            display_name=\"Background Color\",\n            info=\"The background color of the icon.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"chat_icon\",\n            display_name=\"Icon\",\n            info=\"The icon of the message.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"text_color\",\n            display_name=\"Text Color\",\n            info=\"The text color of the name\",\n            advanced=True,\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\"),\n    ]\n\n    def message_response(self) -> Message:\n        _background_color = self.background_color\n        _text_color = self.text_color\n        _icon = self.chat_icon\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n            files=self.files,\n            properties={\"background_color\": _background_color, \"text_color\": _text_color, \"icon\": _icon},\n        )\n        if self.session_id and isinstance(message, Message) and self.should_store_message:\n            stored_message = self.send_message(\n                message,\n            )\n            self.message.value = stored_message\n            message = stored_message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "input_value": {"tool_mode": false, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "K\u00f6nnen Hunde fliegen?", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as input.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}, "sender": {"tool_mode": false, "trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "User", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "User", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "text_color": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "text_color", "value": "", "display_name": "Text Color", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The text color of the name", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Get chat inputs from the Playground.", "icon": "MessagesSquare", "base_classes": ["Message"], "display_name": "Chat Input", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "files", "background_color", "chat_icon", "text_color"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.1"}, "id": "ChatInput-Q0ufi"}, "selected": false, "width": 320, "height": 233, "dragging": false, "positionAbsolute": {"x": -1386.5005184218498, "y": 277.68273814389573}}, {"id": "ChatOutput-yH08p", "type": "genericNode", "position": {"x": 1385.3228364613599, "y": 239.35968523632545}, "data": {"type": "ChatOutput", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.io.chat import ChatComponent\nfrom langflow.inputs import BoolInput\nfrom langflow.io import DropdownInput, MessageTextInput, Output\nfrom langflow.memory import store_message\nfrom langflow.schema.message import Message\nfrom langflow.utils.constants import MESSAGE_SENDER_AI, MESSAGE_SENDER_NAME_AI, MESSAGE_SENDER_USER\n\n\nclass ChatOutput(ChatComponent):\n    display_name = \"Chat Output\"\n    description = \"Display a chat message in the Playground.\"\n    icon = \"ChatOutput\"\n    name = \"ChatOutput\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"input_value\",\n            display_name=\"Text\",\n            info=\"Message to be passed as output.\",\n        ),\n        BoolInput(\n            name=\"should_store_message\",\n            display_name=\"Store Messages\",\n            info=\"Store the message in the history.\",\n            value=True,\n            advanced=True,\n        ),\n        DropdownInput(\n            name=\"sender\",\n            display_name=\"Sender Type\",\n            options=[MESSAGE_SENDER_AI, MESSAGE_SENDER_USER],\n            value=MESSAGE_SENDER_AI,\n            advanced=True,\n            info=\"Type of sender.\",\n        ),\n        MessageTextInput(\n            name=\"sender_name\",\n            display_name=\"Sender Name\",\n            info=\"Name of the sender.\",\n            value=MESSAGE_SENDER_NAME_AI,\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"session_id\",\n            display_name=\"Session ID\",\n            info=\"The session ID of the chat. If empty, the current session ID parameter will be used.\",\n            advanced=True,\n        ),\n        MessageTextInput(\n            name=\"data_template\",\n            display_name=\"Data Template\",\n            value=\"{text}\",\n            advanced=True,\n            info=\"Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.\",\n        ),\n    ]\n    outputs = [\n        Output(display_name=\"Message\", name=\"message\", method=\"message_response\", stream=True),\n    ]\n\n    def message_response(self) -> Message:\n        message = Message(\n            text=self.input_value,\n            sender=self.sender,\n            sender_name=self.sender_name,\n            session_id=self.session_id,\n        )\n        if (\n            self.session_id\n            and isinstance(message, Message)\n            and isinstance(message.text, str)\n            and self.should_store_message\n        ):\n            store_message(\n                message,\n                flow_id=self.graph.flow_id,\n            )\n            self.message.value = message\n\n        self.status = message\n        return message\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "data_template": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "data_template", "value": "{text}", "display_name": "Data Template", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Template to convert Data to Text. If left empty, it will be dynamically set to the Data's text key.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Text", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "Message to be passed as output.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "sender": {"trace_as_metadata": true, "options": ["Machine", "User"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "sender", "value": "Machine", "display_name": "Sender Type", "advanced": true, "dynamic": false, "info": "Type of sender.", "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "sender_name": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sender_name", "value": "AI", "display_name": "Sender Name", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "Name of the sender.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "session_id": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "session_id", "value": "", "display_name": "Session ID", "advanced": true, "input_types": ["Message"], "dynamic": false, "info": "The session ID of the chat. If empty, the current session ID parameter will be used.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "should_store_message": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "should_store_message", "value": true, "display_name": "Store Messages", "advanced": true, "dynamic": false, "info": "Store the message in the history.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Display a chat message in the Playground.", "icon": "ChatOutput", "base_classes": ["Message"], "display_name": "Chat Output", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "message", "display_name": "Message", "method": "message_response", "value": "__UNDEFINED__", "cache": true}], "field_order": ["input_value", "should_store_message", "sender", "sender_name", "session_id", "data_template"], "beta": false, "edited": true, "metadata": {}, "lf_version": "1.1.1"}, "id": "ChatOutput-yH08p"}, "selected": false, "width": 320, "height": 233, "positionAbsolute": {"x": 1385.3228364613599, "y": 239.35968523632545}, "dragging": false}, {"id": "OllamaModel-Q5Wte", "type": "genericNode", "position": {"x": 859.1681341041351, "y": 31.97753011089739}, "data": {"type": "OllamaModel", "node": {"template": {"_type": "Component", "output_parser": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "output_parser", "value": "", "display_name": "Output Parser", "advanced": true, "input_types": ["OutputParser"], "dynamic": false, "info": "The parser to use to parse the output of the model", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "base_url": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "base_url", "value": "http://localhost:11434", "display_name": "Base URL", "advanced": false, "dynamic": false, "info": "Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from typing import Any\nfrom urllib.parse import urljoin\n\nimport httpx\nfrom langchain_ollama import ChatOllama\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import LanguageModel\nfrom langflow.inputs.inputs import HandleInput\nfrom langflow.io import BoolInput, DictInput, DropdownInput, FloatInput, IntInput, StrInput\n\n\nclass ChatOllamaComponent(LCModelComponent):\n    display_name = \"Ollama\"\n    description = \"Generate text using Ollama Local LLMs.\"\n    icon = \"Ollama\"\n    name = \"OllamaModel\"\n\n    def update_build_config(self, build_config: dict, field_value: Any, field_name: str | None = None):\n        if field_name == \"mirostat\":\n            if field_value == \"Disabled\":\n                build_config[\"mirostat_eta\"][\"advanced\"] = True\n                build_config[\"mirostat_tau\"][\"advanced\"] = True\n                build_config[\"mirostat_eta\"][\"value\"] = None\n                build_config[\"mirostat_tau\"][\"value\"] = None\n\n            else:\n                build_config[\"mirostat_eta\"][\"advanced\"] = False\n                build_config[\"mirostat_tau\"][\"advanced\"] = False\n\n                if field_value == \"Mirostat 2.0\":\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.2\n                    build_config[\"mirostat_tau\"][\"value\"] = 10\n                else:\n                    build_config[\"mirostat_eta\"][\"value\"] = 0.1\n                    build_config[\"mirostat_tau\"][\"value\"] = 5\n\n        if field_name == \"model_name\":\n            base_url_dict = build_config.get(\"base_url\", {})\n            base_url_load_from_db = base_url_dict.get(\"load_from_db\", False)\n            base_url_value = base_url_dict.get(\"value\")\n            if base_url_load_from_db:\n                base_url_value = self.variables(base_url_value, field_name)\n            elif not base_url_value:\n                base_url_value = \"http://localhost:11434\"\n            build_config[\"model_name\"][\"options\"] = self.get_model(base_url_value)\n        if field_name == \"keep_alive_flag\":\n            if field_value == \"Keep\":\n                build_config[\"keep_alive\"][\"value\"] = \"-1\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            elif field_value == \"Immediately\":\n                build_config[\"keep_alive\"][\"value\"] = \"0\"\n                build_config[\"keep_alive\"][\"advanced\"] = True\n            else:\n                build_config[\"keep_alive\"][\"advanced\"] = False\n\n        return build_config\n\n    def get_model(self, base_url_value: str) -> list[str]:\n        try:\n            url = urljoin(base_url_value, \"/api/tags\")\n            with httpx.Client() as client:\n                response = client.get(url)\n                response.raise_for_status()\n                data = response.json()\n\n                return [model[\"name\"] for model in data.get(\"models\", [])]\n        except Exception as e:\n            msg = \"Could not retrieve models. Please, make sure Ollama is running.\"\n            raise ValueError(msg) from e\n\n    inputs = [\n        StrInput(\n            name=\"base_url\",\n            display_name=\"Base URL\",\n            info=\"Endpoint of the Ollama API. Defaults to 'http://localhost:11434' if not specified.\",\n            value=\"http://localhost:11434\",\n        ),\n        DropdownInput(\n            name=\"model_name\",\n            display_name=\"Model Name\",\n            value=\"llama3.1\",\n            info=\"Refer to https://ollama.com/library for more models.\",\n            refresh_button=True,\n        ),\n        FloatInput(\n            name=\"temperature\",\n            display_name=\"Temperature\",\n            value=0.2,\n            info=\"Controls the creativity of model responses.\",\n        ),\n        StrInput(\n            name=\"format\", display_name=\"Format\", info=\"Specify the format of the output (e.g., json).\", advanced=True\n        ),\n        DictInput(name=\"metadata\", display_name=\"Metadata\", info=\"Metadata to add to the run trace.\", advanced=True),\n        DropdownInput(\n            name=\"mirostat\",\n            display_name=\"Mirostat\",\n            options=[\"Disabled\", \"Mirostat\", \"Mirostat 2.0\"],\n            info=\"Enable/disable Mirostat sampling for controlling perplexity.\",\n            value=\"Disabled\",\n            advanced=True,\n            real_time_refresh=True,\n        ),\n        FloatInput(\n            name=\"mirostat_eta\",\n            display_name=\"Mirostat Eta\",\n            info=\"Learning rate for Mirostat algorithm. (Default: 0.1)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"mirostat_tau\",\n            display_name=\"Mirostat Tau\",\n            info=\"Controls the balance between coherence and diversity of the output. (Default: 5.0)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_ctx\",\n            display_name=\"Context Window Size\",\n            info=\"Size of the context window for generating tokens. (Default: 2048)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_gpu\",\n            display_name=\"Number of GPUs\",\n            info=\"Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"num_thread\",\n            display_name=\"Number of Threads\",\n            info=\"Number of threads to use during computation. (Default: detected for optimal performance)\",\n            advanced=True,\n        ),\n        IntInput(\n            name=\"repeat_last_n\",\n            display_name=\"Repeat Last N\",\n            info=\"How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)\",\n            advanced=True,\n        ),\n        FloatInput(\n            name=\"repeat_penalty\",\n            display_name=\"Repeat Penalty\",\n            info=\"Penalty for repetitions in generated text. (Default: 1.1)\",\n            advanced=True,\n        ),\n        FloatInput(name=\"tfs_z\", display_name=\"TFS Z\", info=\"Tail free sampling value. (Default: 1)\", advanced=True),\n        IntInput(name=\"timeout\", display_name=\"Timeout\", info=\"Timeout for the request stream.\", advanced=True),\n        IntInput(\n            name=\"top_k\", display_name=\"Top K\", info=\"Limits token selection to top K. (Default: 40)\", advanced=True\n        ),\n        FloatInput(name=\"top_p\", display_name=\"Top P\", info=\"Works together with top-k. (Default: 0.9)\", advanced=True),\n        BoolInput(name=\"verbose\", display_name=\"Verbose\", info=\"Whether to print out response text.\", advanced=True),\n        StrInput(\n            name=\"tags\",\n            display_name=\"Tags\",\n            info=\"Comma-separated list of tags to add to the run trace.\",\n            advanced=True,\n        ),\n        StrInput(\n            name=\"stop_tokens\",\n            display_name=\"Stop Tokens\",\n            info=\"Comma-separated list of tokens to signal the model to stop generating text.\",\n            advanced=True,\n        ),\n        StrInput(name=\"system\", display_name=\"System\", info=\"System to use for generating text.\", advanced=True),\n        StrInput(name=\"template\", display_name=\"Template\", info=\"Template to use for generating text.\", advanced=True),\n        HandleInput(\n            name=\"output_parser\",\n            display_name=\"Output Parser\",\n            info=\"The parser to use to parse the output of the model\",\n            advanced=True,\n            input_types=[\"OutputParser\"],\n        ),\n        *LCModelComponent._base_inputs,\n    ]\n\n    def build_model(self) -> LanguageModel:  # type: ignore[type-var]\n        # Mapping mirostat settings to their corresponding values\n        mirostat_options = {\"Mirostat\": 1, \"Mirostat 2.0\": 2}\n\n        # Default to 0 for 'Disabled'\n        mirostat_value = mirostat_options.get(self.mirostat, 0)\n\n        # Set mirostat_eta and mirostat_tau to None if mirostat is disabled\n        if mirostat_value == 0:\n            mirostat_eta = None\n            mirostat_tau = None\n        else:\n            mirostat_eta = self.mirostat_eta\n            mirostat_tau = self.mirostat_tau\n\n        # Mapping system settings to their corresponding values\n        llm_params = {\n            \"base_url\": self.base_url,\n            \"model\": self.model_name,\n            \"mirostat\": mirostat_value,\n            \"format\": self.format,\n            \"metadata\": self.metadata,\n            \"tags\": self.tags.split(\",\") if self.tags else None,\n            \"mirostat_eta\": mirostat_eta,\n            \"mirostat_tau\": mirostat_tau,\n            \"num_ctx\": self.num_ctx or None,\n            \"num_gpu\": self.num_gpu or None,\n            \"num_thread\": self.num_thread or None,\n            \"repeat_last_n\": self.repeat_last_n or None,\n            \"repeat_penalty\": self.repeat_penalty or None,\n            \"temperature\": self.temperature or None,\n            \"stop\": self.stop_tokens.split(\",\") if self.stop_tokens else None,\n            \"system\": self.system,\n            \"template\": self.template,\n            \"tfs_z\": self.tfs_z or None,\n            \"timeout\": self.timeout or None,\n            \"top_k\": self.top_k or None,\n            \"top_p\": self.top_p or None,\n            \"verbose\": self.verbose,\n        }\n\n        # Remove parameters with None values\n        llm_params = {k: v for k, v in llm_params.items() if v is not None}\n\n        try:\n            output = ChatOllama(**llm_params)\n        except Exception as e:\n            msg = \"Could not initialize Ollama LLM.\"\n            raise ValueError(msg) from e\n\n        return output\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "format": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "format", "value": "", "display_name": "Format", "advanced": true, "dynamic": false, "info": "Specify the format of the output (e.g., json).", "title_case": false, "type": "str", "_input_type": "StrInput"}, "input_value": {"trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "input_value", "value": "", "display_name": "Input", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageInput"}, "metadata": {"trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "metadata", "value": {}, "display_name": "Metadata", "advanced": true, "dynamic": false, "info": "Metadata to add to the run trace.", "title_case": false, "type": "dict", "_input_type": "DictInput"}, "mirostat": {"tool_mode": false, "trace_as_metadata": true, "options": ["Disabled", "Mirostat", "Mirostat 2.0"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "mirostat", "value": "Disabled", "display_name": "Mirostat", "advanced": true, "dynamic": false, "info": "Enable/disable Mirostat sampling for controlling perplexity.", "real_time_refresh": true, "title_case": false, "type": "str", "_input_type": "DropdownInput"}, "mirostat_eta": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_eta", "value": "", "display_name": "Mirostat Eta", "advanced": true, "dynamic": false, "info": "Learning rate for Mirostat algorithm. (Default: 0.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "mirostat_tau": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "mirostat_tau", "value": "", "display_name": "Mirostat Tau", "advanced": true, "dynamic": false, "info": "Controls the balance between coherence and diversity of the output. (Default: 5.0)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "model_name": {"tool_mode": false, "trace_as_metadata": true, "options": ["tinyllama:1.1b-chat-v0.6-q3_K_S", "llama3.2:1b", "all-minilm:22m", "llama3.2:3b"], "combobox": false, "required": false, "placeholder": "", "show": true, "name": "model_name", "value": "llama3.2:3b", "display_name": "Model Name", "advanced": false, "dynamic": false, "info": "Refer to https://ollama.com/library for more models.", "refresh_button": true, "title_case": false, "type": "str", "_input_type": "DropdownInput", "load_from_db": false}, "num_ctx": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_ctx", "value": "", "display_name": "Context Window Size", "advanced": true, "dynamic": false, "info": "Size of the context window for generating tokens. (Default: 2048)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_gpu": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_gpu", "value": "", "display_name": "Number of GPUs", "advanced": true, "dynamic": false, "info": "Number of GPUs to use for computation. (Default: 1 on macOS, 0 to disable)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "num_thread": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "num_thread", "value": "", "display_name": "Number of Threads", "advanced": true, "dynamic": false, "info": "Number of threads to use during computation. (Default: detected for optimal performance)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_last_n": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_last_n", "value": "", "display_name": "Repeat Last N", "advanced": true, "dynamic": false, "info": "How far back the model looks to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "repeat_penalty": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "repeat_penalty", "value": "", "display_name": "Repeat Penalty", "advanced": true, "dynamic": false, "info": "Penalty for repetitions in generated text. (Default: 1.1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "stop_tokens": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "stop_tokens", "value": "", "display_name": "Stop Tokens", "advanced": true, "dynamic": false, "info": "Comma-separated list of tokens to signal the model to stop generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "stream": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "stream", "value": false, "display_name": "Stream", "advanced": false, "dynamic": false, "info": "Stream the response from the model. Streaming works only in Chat.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "system": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system", "value": "", "display_name": "System", "advanced": true, "dynamic": false, "info": "System to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "system_message": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "system_message", "value": "", "display_name": "System Message", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "System message to pass to the model.", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "tags": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "tags", "value": "", "display_name": "Tags", "advanced": true, "dynamic": false, "info": "Comma-separated list of tags to add to the run trace.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "temperature": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "temperature", "value": 0.2, "display_name": "Temperature", "advanced": false, "dynamic": false, "info": "Controls the creativity of model responses.", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "template": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "", "display_name": "Template", "advanced": true, "dynamic": false, "info": "Template to use for generating text.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "tfs_z": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "tfs_z", "value": "", "display_name": "TFS Z", "advanced": true, "dynamic": false, "info": "Tail free sampling value. (Default: 1)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "timeout": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "timeout", "value": "", "display_name": "Timeout", "advanced": true, "dynamic": false, "info": "Timeout for the request stream.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_k": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_k", "value": "", "display_name": "Top K", "advanced": true, "dynamic": false, "info": "Limits token selection to top K. (Default: 40)", "title_case": false, "type": "int", "_input_type": "IntInput"}, "top_p": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "top_p", "value": "", "display_name": "Top P", "advanced": true, "dynamic": false, "info": "Works together with top-k. (Default: 0.9)", "title_case": false, "type": "float", "_input_type": "FloatInput"}, "verbose": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "verbose", "value": false, "display_name": "Verbose", "advanced": true, "dynamic": false, "info": "Whether to print out response text.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}}, "description": "Generate text using Ollama Local LLMs.", "icon": "Ollama", "base_classes": ["LanguageModel", "Message"], "display_name": "Ollama", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text_output", "hidden": null, "display_name": "Text", "method": "text_response", "value": "__UNDEFINED__", "cache": true, "required_inputs": []}, {"types": ["LanguageModel"], "selected": "LanguageModel", "name": "model_output", "hidden": null, "display_name": "Language Model", "method": "build_model", "value": "__UNDEFINED__", "cache": true, "required_inputs": []}], "field_order": ["base_url", "model_name", "temperature", "format", "metadata", "mirostat", "mirostat_eta", "mirostat_tau", "num_ctx", "num_gpu", "num_thread", "repeat_last_n", "repeat_penalty", "tfs_z", "timeout", "top_k", "top_p", "verbose", "tags", "stop_tokens", "system", "template", "output_parser", "input_value", "system_message", "stream"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.1"}, "id": "OllamaModel-Q5Wte"}, "selected": false, "width": 320, "height": 670, "positionAbsolute": {"x": 859.1681341041351, "y": 31.97753011089739}, "dragging": false}, {"id": "Prompt-z46JK", "type": "genericNode", "position": {"x": 330.359006750509, "y": 130.7946695815798}, "data": {"type": "Prompt", "node": {"template": {"_type": "Component", "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.base.prompts.api_utils import process_prompt_template\nfrom langflow.custom import Component\nfrom langflow.inputs.inputs import DefaultPromptField\nfrom langflow.io import Output, PromptInput\nfrom langflow.schema.message import Message\nfrom langflow.template.utils import update_template_values\n\n\nclass PromptComponent(Component):\n    display_name: str = \"Prompt\"\n    description: str = \"Create a prompt template with dynamic variables.\"\n    icon = \"prompts\"\n    trace_type = \"prompt\"\n    name = \"Prompt\"\n\n    inputs = [\n        PromptInput(name=\"template\", display_name=\"Template\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Prompt Message\", name=\"prompt\", method=\"build_prompt\"),\n    ]\n\n    async def build_prompt(self) -> Message:\n        prompt = Message.from_template(**self._attributes)\n        self.status = prompt.text\n        return prompt\n\n    def _update_template(self, frontend_node: dict):\n        prompt_template = frontend_node[\"template\"][\"template\"][\"value\"]\n        custom_fields = frontend_node[\"custom_fields\"]\n        frontend_node_template = frontend_node[\"template\"]\n        _ = process_prompt_template(\n            template=prompt_template,\n            name=\"template\",\n            custom_fields=custom_fields,\n            frontend_node_template=frontend_node_template,\n        )\n        return frontend_node\n\n    def post_code_processing(self, new_frontend_node: dict, current_frontend_node: dict):\n        \"\"\"This function is called after the code validation is done.\"\"\"\n        frontend_node = super().post_code_processing(new_frontend_node, current_frontend_node)\n        template = frontend_node[\"template\"][\"template\"][\"value\"]\n        # Kept it duplicated for backwards compatibility\n        _ = process_prompt_template(\n            template=template,\n            name=\"template\",\n            custom_fields=frontend_node[\"custom_fields\"],\n            frontend_node_template=frontend_node[\"template\"],\n        )\n        # Now that template is updated, we need to grab any values that were set in the current_frontend_node\n        # and update the frontend_node with those values\n        update_template_values(new_template=frontend_node, previous_template=current_frontend_node[\"template\"])\n        return frontend_node\n\n    def _get_fallback_input(self, **kwargs):\n        return DefaultPromptField(**kwargs)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "template": {"tool_mode": false, "trace_as_input": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "Context:\n{context}\n\nAntworte in der Sprache in der die Frage gestellt wird.\nUse the language used for the question for your answer.\n\nQuestion:\n{question}", "display_name": "Template", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "prompt", "_input_type": "PromptInput", "load_from_db": false}, "context": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "context", "display_name": "context", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}, "question": {"field_type": "str", "required": false, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "", "fileTypes": [], "file_path": "", "name": "question", "display_name": "question", "advanced": false, "input_types": ["Message", "Text"], "dynamic": false, "info": "", "load_from_db": false, "title_case": false, "type": "str"}}, "description": "Create a prompt template with dynamic variables.", "icon": "prompts", "is_input": null, "is_output": null, "is_composition": null, "base_classes": ["Message"], "name": "", "display_name": "Prompt", "documentation": "", "custom_fields": {"template": ["context", "question"]}, "output_types": [], "full_path": null, "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "prompt", "hidden": null, "display_name": "Prompt Message", "method": "build_prompt", "value": "__UNDEFINED__", "cache": true, "required_inputs": null}], "field_order": ["template"], "beta": false, "legacy": false, "error": null, "edited": false, "metadata": {}, "tool_mode": false}, "id": "Prompt-z46JK"}, "selected": true, "width": 320, "height": 432, "dragging": false, "positionAbsolute": {"x": 330.359006750509, "y": 130.7946695815798}}, {"id": "ParseData-9TLo9", "type": "genericNode", "position": {"x": -165.72084791004886, "y": 29.39511479057296}, "data": {"type": "ParseData", "node": {"template": {"_type": "Component", "data": {"trace_as_metadata": true, "list": false, "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "data", "value": "", "display_name": "Data", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "The data to convert to text.", "title_case": false, "type": "other", "_input_type": "DataInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langflow.custom import Component\nfrom langflow.helpers.data import data_to_text\nfrom langflow.io import DataInput, MultilineInput, Output, StrInput\nfrom langflow.schema.message import Message\n\n\nclass ParseDataComponent(Component):\n    display_name = \"Parse Data\"\n    description = \"Convert Data into plain text following a specified template.\"\n    icon = \"braces\"\n    name = \"ParseData\"\n\n    inputs = [\n        DataInput(name=\"data\", display_name=\"Data\", info=\"The data to convert to text.\"),\n        MultilineInput(\n            name=\"template\",\n            display_name=\"Template\",\n            info=\"The template to use for formatting the data. \"\n            \"It can contain the keys {text}, {data} or any other key in the Data.\",\n            value=\"{text}\",\n        ),\n        StrInput(name=\"sep\", display_name=\"Separator\", advanced=True, value=\"\\n\"),\n    ]\n\n    outputs = [\n        Output(display_name=\"Text\", name=\"text\", method=\"parse_data\"),\n    ]\n\n    def parse_data(self) -> Message:\n        data = self.data if isinstance(self.data, list) else [self.data]\n        template = self.template\n\n        result_string = data_to_text(template, data, sep=self.sep)\n        self.status = result_string\n        return Message(text=result_string)\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "sep": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "sep", "value": "\n", "display_name": "Separator", "advanced": true, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "StrInput"}, "template": {"trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "template", "value": "{text}", "display_name": "Template", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "The template to use for formatting the data. It can contain the keys {text}, {data} or any other key in the Data.", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "Convert Data into plain text following a specified template.", "icon": "braces", "base_classes": ["Message"], "display_name": "Parse Data", "documentation": "", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Message"], "selected": "Message", "name": "text", "display_name": "Text", "method": "parse_data", "value": "__UNDEFINED__", "cache": true}], "field_order": ["data", "template", "sep"], "beta": false, "edited": false, "metadata": {}, "lf_version": "1.1.1"}, "id": "ParseData-9TLo9"}, "selected": false, "width": 320, "height": 301, "positionAbsolute": {"x": -165.72084791004886, "y": 29.39511479057296}, "dragging": false}, {"id": "OllamaEmbeddings-UtRPU", "type": "genericNode", "position": {"x": -1364.3879622805493, "y": -222.35075113135755}, "data": {"type": "OllamaEmbeddings", "node": {"template": {"_type": "Component", "base_url": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "base_url", "value": "http://localhost:11434", "display_name": "Ollama Base URL", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_ollama import OllamaEmbeddings\n\nfrom langflow.base.models.model import LCModelComponent\nfrom langflow.field_typing import Embeddings\nfrom langflow.io import MessageTextInput, Output\n\n\nclass OllamaEmbeddingsComponent(LCModelComponent):\n    display_name: str = \"Ollama Embeddings\"\n    description: str = \"Generate embeddings using Ollama models.\"\n    documentation = \"https://python.langchain.com/docs/integrations/text_embedding/ollama\"\n    icon = \"Ollama\"\n    name = \"OllamaEmbeddings\"\n\n    inputs = [\n        MessageTextInput(\n            name=\"model\",\n            display_name=\"Ollama Model\",\n            value=\"nomic-embed-text\",\n        ),\n        MessageTextInput(\n            name=\"base_url\",\n            display_name=\"Ollama Base URL\",\n            value=\"http://localhost:11434\",\n        ),\n    ]\n\n    outputs = [\n        Output(display_name=\"Embeddings\", name=\"embeddings\", method=\"build_embeddings\"),\n    ]\n\n    def build_embeddings(self) -> Embeddings:\n        try:\n            output = OllamaEmbeddings(model=self.model, base_url=self.base_url)\n        except Exception as e:\n            msg = \"Could not connect to Ollama API.\"\n            raise ValueError(msg) from e\n        return output\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "model": {"tool_mode": false, "trace_as_input": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "model", "value": "all-minilm:22m", "display_name": "Ollama Model", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MessageTextInput"}}, "description": "Generate embeddings using Ollama models.", "icon": "Ollama", "base_classes": ["Embeddings"], "display_name": "Ollama Embeddings", "documentation": "https://python.langchain.com/docs/integrations/text_embedding/ollama", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Embeddings"], "selected": "Embeddings", "name": "embeddings", "display_name": "Embeddings", "method": "build_embeddings", "value": "__UNDEFINED__", "cache": true}], "field_order": ["model", "base_url"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.1"}, "id": "OllamaEmbeddings-UtRPU"}, "selected": false, "width": 320, "height": 320, "positionAbsolute": {"x": -1364.3879622805493, "y": -222.35075113135755}, "dragging": false}, {"id": "FAISS-NHMuV", "type": "genericNode", "position": {"x": -777.1066368445192, "y": -210.70686682355532}, "data": {"node": {"template": {"_type": "Component", "embedding": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "embedding", "value": "", "display_name": "Embedding", "advanced": false, "input_types": ["Embeddings"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "HandleInput"}, "ingest_data": {"tool_mode": false, "trace_as_metadata": true, "list": true, "trace_as_input": true, "required": false, "placeholder": "", "show": true, "name": "ingest_data", "value": "", "display_name": "Ingest Data", "advanced": false, "input_types": ["Data"], "dynamic": false, "info": "", "title_case": false, "type": "other", "_input_type": "DataInput"}, "allow_dangerous_deserialization": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "allow_dangerous_deserialization", "value": true, "display_name": "Allow Dangerous Deserialization", "advanced": true, "dynamic": false, "info": "Set to True to allow loading pickle files from untrusted sources. Only enable this if you trust the source of the data.", "title_case": false, "type": "bool", "_input_type": "BoolInput"}, "code": {"type": "code", "required": true, "placeholder": "", "list": false, "show": true, "multiline": true, "value": "from langchain_community.vectorstores import FAISS\nfrom loguru import logger\n\nfrom langflow.base.vectorstores.model import LCVectorStoreComponent, check_cached_vector_store\nfrom langflow.helpers.data import docs_to_data\nfrom langflow.io import BoolInput, DataInput, HandleInput, IntInput, MultilineInput, StrInput\nfrom langflow.schema import Data\n\n\nclass FaissVectorStoreComponent(LCVectorStoreComponent):\n    \"\"\"FAISS Vector Store with search capabilities.\"\"\"\n\n    display_name: str = \"FAISS\"\n    description: str = \"FAISS Vector Store with search capabilities\"\n    documentation = \"https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss\"\n    name = \"FAISS\"\n    icon = \"FAISS\"\n\n    inputs = [\n        StrInput(\n            name=\"index_name\",\n            display_name=\"Index Name\",\n            value=\"langflow_index\",\n        ),\n        StrInput(\n            name=\"persist_directory\",\n            display_name=\"Persist Directory\",\n            info=\"Path to save the FAISS index. It will be relative to where Langflow is running.\",\n        ),\n        MultilineInput(\n            name=\"search_query\",\n            display_name=\"Search Query\",\n        ),\n        DataInput(\n            name=\"ingest_data\",\n            display_name=\"Ingest Data\",\n            is_list=True,\n        ),\n        BoolInput(\n            name=\"allow_dangerous_deserialization\",\n            display_name=\"Allow Dangerous Deserialization\",\n            info=\"Set to True to allow loading pickle files from untrusted sources. \"\n            \"Only enable this if you trust the source of the data.\",\n            advanced=True,\n            value=True,\n        ),\n        HandleInput(name=\"embedding\", display_name=\"Embedding\", input_types=[\"Embeddings\"]),\n        IntInput(\n            name=\"number_of_results\",\n            display_name=\"Number of Results\",\n            info=\"Number of results to return.\",\n            advanced=True,\n            value=4,\n        ),\n    ]\n\n    @check_cached_vector_store\n    def build_vector_store(self) -> FAISS:\n        \"\"\"Builds the FAISS object.\"\"\"\n        if not self.persist_directory:\n            msg = \"Folder path is required to save the FAISS index.\"\n            raise ValueError(msg)\n        path = self.resolve_path(self.persist_directory)\n\n        documents = []\n\n        for _input in self.ingest_data or []:\n            if isinstance(_input, Data):\n                documents.append(_input.to_lc_document())\n            else:\n                documents.append(_input)\n\n        faiss = FAISS.from_documents(documents=documents, embedding=self.embedding)\n        faiss.save_local(str(path), self.index_name)\n\n        return faiss\n\n    def search_documents(self) -> list[Data]:\n        \"\"\"Search for documents in the FAISS vector store.\"\"\"\n        if not self.persist_directory:\n            msg = \"Folder path is required to load the FAISS index.\"\n            raise ValueError(msg)\n        path = self.resolve_path(self.persist_directory)\n\n        vector_store = FAISS.load_local(\n            folder_path=path,\n            embeddings=self.embedding,\n            index_name=self.index_name,\n            allow_dangerous_deserialization=self.allow_dangerous_deserialization,\n        )\n\n        if not vector_store:\n            msg = \"Failed to load the FAISS index.\"\n            raise ValueError(msg)\n\n        logger.debug(f\"Search input: {self.search_query}\")\n        logger.debug(f\"Number of results: {self.number_of_results}\")\n\n        if self.search_query and isinstance(self.search_query, str) and self.search_query.strip():\n            docs = vector_store.similarity_search(\n                query=self.search_query,\n                k=self.number_of_results,\n            )\n\n            logger.debug(f\"Retrieved documents: {len(docs)}\")\n\n            data = docs_to_data(docs)\n            logger.debug(f\"Converted documents to data: {len(data)}\")\n            logger.debug(data)\n            return data  # Return the search results data\n        logger.debug(\"No search input provided. Skipping search.\")\n        return []\n", "fileTypes": [], "file_path": "", "password": false, "name": "code", "advanced": true, "dynamic": true, "info": "", "load_from_db": false, "title_case": false}, "index_name": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "index_name", "value": "langflow_vs", "display_name": "Index Name", "advanced": false, "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "StrInput"}, "number_of_results": {"trace_as_metadata": true, "list": false, "required": false, "placeholder": "", "show": true, "name": "number_of_results", "value": 4, "display_name": "Number of Results", "advanced": true, "dynamic": false, "info": "Number of results to return.", "title_case": false, "type": "int", "_input_type": "IntInput"}, "persist_directory": {"trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "persist_directory", "value": "C:\\Users\\susan\\Documents\\repos\\offline_chatbot\\Final_Project\\VectorStores\\langflow", "display_name": "Persist Directory", "advanced": false, "dynamic": false, "info": "Path to save the FAISS index. It will be relative to where Langflow is running.", "title_case": false, "type": "str", "_input_type": "StrInput"}, "search_query": {"tool_mode": false, "trace_as_input": true, "multiline": true, "trace_as_metadata": true, "load_from_db": false, "list": false, "required": false, "placeholder": "", "show": true, "name": "search_query", "value": "", "display_name": "Search Query", "advanced": false, "input_types": ["Message"], "dynamic": false, "info": "", "title_case": false, "type": "str", "_input_type": "MultilineInput"}}, "description": "FAISS Vector Store with search capabilities", "icon": "FAISS", "base_classes": ["Data", "Retriever"], "display_name": "FAISS", "documentation": "https://python.langchain.com/docs/modules/data_connection/vectorstores/integrations/faiss", "custom_fields": {}, "output_types": [], "pinned": false, "conditional_paths": [], "frozen": false, "outputs": [{"types": ["Retriever"], "selected": "Retriever", "name": "base_retriever", "display_name": "Retriever", "method": "build_base_retriever", "value": "__UNDEFINED__", "cache": true, "required_inputs": []}, {"types": ["Data"], "selected": "Data", "name": "search_results", "display_name": "Search Results", "method": "search_documents", "value": "__UNDEFINED__", "cache": true, "required_inputs": []}], "field_order": ["index_name", "persist_directory", "search_query", "ingest_data", "allow_dangerous_deserialization", "embedding", "number_of_results"], "beta": false, "legacy": false, "edited": false, "metadata": {}, "tool_mode": false, "lf_version": "1.1.1"}, "type": "FAISS", "id": "FAISS-NHMuV"}, "selected": false, "width": 320, "height": 550, "positionAbsolute": {"x": -777.1066368445192, "y": -210.70686682355532}, "dragging": false}], "edges": [{"source": "ChatInput-Q0ufi", "sourceHandle": "{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-Q0ufi\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-z46JK", "targetHandle": "{\u0153fieldName\u0153:\u0153question\u0153,\u0153id\u0153:\u0153Prompt-z46JK\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "question", "id": "Prompt-z46JK", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "ChatInput", "id": "ChatInput-Q0ufi", "name": "message", "output_types": ["Message"]}}, "id": "reactflow__edge-ChatInput-Q0ufi{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-Q0ufi\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-z46JK{\u0153fieldName\u0153:\u0153question\u0153,\u0153id\u0153:\u0153Prompt-z46JK\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "ParseData-9TLo9", "sourceHandle": "{\u0153dataType\u0153:\u0153ParseData\u0153,\u0153id\u0153:\u0153ParseData-9TLo9\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "Prompt-z46JK", "targetHandle": "{\u0153fieldName\u0153:\u0153context\u0153,\u0153id\u0153:\u0153Prompt-z46JK\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "context", "id": "Prompt-z46JK", "inputTypes": ["Message", "Text"], "type": "str"}, "sourceHandle": {"dataType": "ParseData", "id": "ParseData-9TLo9", "name": "text", "output_types": ["Message"]}}, "id": "reactflow__edge-ParseData-9TLo9{\u0153dataType\u0153:\u0153ParseData\u0153,\u0153id\u0153:\u0153ParseData-9TLo9\u0153,\u0153name\u0153:\u0153text\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-Prompt-z46JK{\u0153fieldName\u0153:\u0153context\u0153,\u0153id\u0153:\u0153Prompt-z46JK\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153,\u0153Text\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "Prompt-z46JK", "sourceHandle": "{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-z46JK\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "OllamaModel-Q5Wte", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-Q5Wte\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "OllamaModel-Q5Wte", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "Prompt", "id": "Prompt-z46JK", "name": "prompt", "output_types": ["Message"]}}, "id": "reactflow__edge-Prompt-z46JK{\u0153dataType\u0153:\u0153Prompt\u0153,\u0153id\u0153:\u0153Prompt-z46JK\u0153,\u0153name\u0153:\u0153prompt\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-OllamaModel-Q5Wte{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153OllamaModel-Q5Wte\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": "", "animated": false}, {"source": "OllamaModel-Q5Wte", "sourceHandle": "{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-Q5Wte\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "ChatOutput-yH08p", "targetHandle": "{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-yH08p\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "input_value", "id": "ChatOutput-yH08p", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "OllamaModel", "id": "OllamaModel-Q5Wte", "name": "text_output", "output_types": ["Message"]}}, "id": "reactflow__edge-OllamaModel-Q5Wte{\u0153dataType\u0153:\u0153OllamaModel\u0153,\u0153id\u0153:\u0153OllamaModel-Q5Wte\u0153,\u0153name\u0153:\u0153text_output\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-ChatOutput-yH08p{\u0153fieldName\u0153:\u0153input_value\u0153,\u0153id\u0153:\u0153ChatOutput-yH08p\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "className": "", "animated": false}, {"source": "ChatInput-Q0ufi", "sourceHandle": "{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-Q0ufi\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}", "target": "FAISS-NHMuV", "targetHandle": "{\u0153fieldName\u0153:\u0153search_query\u0153,\u0153id\u0153:\u0153FAISS-NHMuV\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "data": {"targetHandle": {"fieldName": "search_query", "id": "FAISS-NHMuV", "inputTypes": ["Message"], "type": "str"}, "sourceHandle": {"dataType": "ChatInput", "id": "ChatInput-Q0ufi", "name": "message", "output_types": ["Message"]}}, "id": "reactflow__edge-ChatInput-Q0ufi{\u0153dataType\u0153:\u0153ChatInput\u0153,\u0153id\u0153:\u0153ChatInput-Q0ufi\u0153,\u0153name\u0153:\u0153message\u0153,\u0153output_types\u0153:[\u0153Message\u0153]}-FAISS-NHMuV{\u0153fieldName\u0153:\u0153search_query\u0153,\u0153id\u0153:\u0153FAISS-NHMuV\u0153,\u0153inputTypes\u0153:[\u0153Message\u0153],\u0153type\u0153:\u0153str\u0153}", "animated": false, "className": ""}, {"source": "OllamaEmbeddings-UtRPU", "sourceHandle": "{\u0153dataType\u0153:\u0153OllamaEmbeddings\u0153,\u0153id\u0153:\u0153OllamaEmbeddings-UtRPU\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}", "target": "FAISS-NHMuV", "targetHandle": "{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153FAISS-NHMuV\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "embedding", "id": "FAISS-NHMuV", "inputTypes": ["Embeddings"], "type": "other"}, "sourceHandle": {"dataType": "OllamaEmbeddings", "id": "OllamaEmbeddings-UtRPU", "name": "embeddings", "output_types": ["Embeddings"]}}, "id": "reactflow__edge-OllamaEmbeddings-UtRPU{\u0153dataType\u0153:\u0153OllamaEmbeddings\u0153,\u0153id\u0153:\u0153OllamaEmbeddings-UtRPU\u0153,\u0153name\u0153:\u0153embeddings\u0153,\u0153output_types\u0153:[\u0153Embeddings\u0153]}-FAISS-NHMuV{\u0153fieldName\u0153:\u0153embedding\u0153,\u0153id\u0153:\u0153FAISS-NHMuV\u0153,\u0153inputTypes\u0153:[\u0153Embeddings\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": ""}, {"source": "FAISS-NHMuV", "sourceHandle": "{\u0153dataType\u0153:\u0153FAISS\u0153,\u0153id\u0153:\u0153FAISS-NHMuV\u0153,\u0153name\u0153:\u0153search_results\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}", "target": "ParseData-9TLo9", "targetHandle": "{\u0153fieldName\u0153:\u0153data\u0153,\u0153id\u0153:\u0153ParseData-9TLo9\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "data": {"targetHandle": {"fieldName": "data", "id": "ParseData-9TLo9", "inputTypes": ["Data"], "type": "other"}, "sourceHandle": {"dataType": "FAISS", "id": "FAISS-NHMuV", "name": "search_results", "output_types": ["Data"]}}, "id": "reactflow__edge-FAISS-NHMuV{\u0153dataType\u0153:\u0153FAISS\u0153,\u0153id\u0153:\u0153FAISS-NHMuV\u0153,\u0153name\u0153:\u0153search_results\u0153,\u0153output_types\u0153:[\u0153Data\u0153]}-ParseData-9TLo9{\u0153fieldName\u0153:\u0153data\u0153,\u0153id\u0153:\u0153ParseData-9TLo9\u0153,\u0153inputTypes\u0153:[\u0153Data\u0153],\u0153type\u0153:\u0153other\u0153}", "animated": false, "className": ""}], "viewport": {"x": 106.18860848606255, "y": 92.80584312479232, "zoom": 0.6343204648262649}}, "is_component": false, "updated_at": "2024-12-28T07:25:38+00:00", "webhook": false, "endpoint_name": "chat", "tags": null, "id": "21904430-bfc5-469a-aa86-f5df72b595e7", "user_id": "ee2ab924-6c8f-402b-94f9-03dfe7fb6e1e", "folder_id": "de943c14-ac97-4afa-aee7-43500e2a68ce"}